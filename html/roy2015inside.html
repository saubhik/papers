<!DOCTYPE html>
<html>
<head>
  <title>Papers</title>
  <link href='../css/style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
</head>

<body>
  <div id=header>
    <a href="../">Papers</a>
  </div>
  <div id="container">
<h1 id="inside-the-social-networks-datacenter-network">Inside the Social
Network&#x2019;s (Datacenter) Network</h1>
<p>Paper: <a href="https://dl.acm.org/doi/10.1145/2785956.2787472">Arjun
Roy, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C. Snoeren.
2015. Inside the Social Network&#x2019;s (Datacenter) Network. In Proceedings
of the 2015 ACM Conference on Special Interest Group on Data
Communication (SIGCOMM &#x2019;15). Association for Computing Machinery, New
York, NY, USA, 123&#x2013;137.
DOI:https://doi.org/10.1145/2785956.2787472)</a></p>
<!--

Contents:
- [ ] Abstract
- [ ] Introduction
- [ ] Related Work
- [ ] A Facebook DC
    - [ ] DC topology
    - [ ] Constituent services
    - [ ] Data collection
        - [ ] Fbflow
        - [ ] Port mirroring
- [ ] Provisioning
    - [ ] Utilization
    - [ ] Locality and stability
    - [ ] Traffic matrix
    - [ ] Implications for connection fabrics
- [ ] Traffic Engineering
    - [ ] Flow characteristics
    - [ ] Load balancing
    - [ ] Heavy hitters
    - [ ] Implications for traffic engineering
- [ ] Switching
    - [ ] Per-packet features
    - [ ] Arrival patterns
    - [ ] Buffer utilization
    - [ ] Concurrent flows
- [ ] Conclusion

-->
<h2 id="summary">Summary</h2>
<p>This paper reports on properties of network traffic in Facebook&#x2019;s
datacenters such as locality, stability and predictibility, and also
discusses the implications for network architecture, traffic
engineering, and switch design. The paper also finds that while
traditional datacenter services like Hadoop follow behaviors reported
earlier, the core Web service and supporting cache infrastructure
exhibit a number of behaviors that contrast with those reported in the
literature. This study is the first to report on production traffic in a
datacenter network connecting hundreds of thousands of 10-Gbps
nodes.</p>
<p>Using both Facebook-wide monitoring systems and per-host
packet-header traces, the study examine services that generate the
majority of the traffic in Facebook&#x2019;s network. Major findings in this
study are:</p>
<ul>
<li>Traffic is neither rack-local nor all-to-all. Locality depends upon
the service but is stable across time periods from seconds to days.
Efficient fabrics may benefit from variable degrees of oversubsription
and less intra-rack bandwidth than typically deployed.
<ul>
<li>This contrasts previously published data that 50-80% of traffic is
rack-local.</li>
<li>This potentially impacts datacenter fabrics</li>
</ul></li>
<li>Many flows are long-lived but not very heavy. Load balancing
distributes traffic across hosts effectively making traffic demands
quite stable over even sub-second intervals. As a result, heavy hitters
are not much larger than the median flow, and the set of heavy hitters
changes rapidly. Instantaneously heavy hitters are frequently not heavy
over longer time periods and this confounds many approaches to traffic
engineering.
<ul>
<li>This contrasts previously published data that demain is frequently
concentrated and bursty.</li>
<li>This potentially impacts traffic engineering.</li>
</ul></li>
<li>Packets are small (median length for non-Hadoop traffic is less than
200 bytes) and do not exhibit on/off arrival behavior. Servers
communicate with 100s of hosts and racks concurrently (within the same
5-ms interval), but the majority of traffic is often destined to (few)
10s of racks.
<ul>
<li>This contrasts previously published data of bimodal ACK/MTU packet
size, on/off arrival behavior, and &lt;5 concurrent large flows.</li>
<li>This potentially impacts SDN controllers, circuit/hybrid
switching.</li>
</ul></li>
</ul>
<h3 id="datacenter-topology">Datacenter Topology</h3>
<ul>
<li>Facebook&#x2019;s network consists of multiple datacenter sites and a
backbone connecting these sites. Each datacenter site contains one or
more datacenter buildings where each datacenter contains multiple
clusters. A cluster is a unit of deployment.</li>
<li>Machines are organized into racks and connected to a top-of-rack
switch (RSW) via 10-Gbps Ethernet links. The number of machines per rack
varies from cluster to cluster.</li>
<li>Each RSW is connected by 10-Gbps links to four aggregation switches
called cluster switches (CSWs). All racks served by a particular set of
CSWs are said to be in the same cluster.</li>
<li>Clusters may be homogeneous in terms of machines. e.g.&#xA0;cache
clusters, or heterogeneous, e.g.&#xA0;Frontend clusters which contain a
mixture of Web servers, load balancers, and cache servers.</li>
<li>CSWs are connected to each other via another layer of aggregation
switches called Fat Cats (FC). This design follows directly from the
need to support a high amount of intra-cluster traffic.</li>
<li>CSWs also connect to aggregation switches for intra-site
(inter-datacenter) traffic and datacenter routers for inter-site
traffic.</li>
</ul>
<p>Every machine typically has one role:</p>
<ul>
<li>Web servers serve Web traffic</li>
<li>MySQL servers store user data</li>
<li>query results are stored temporarily in cache servers including
<ul>
<li>leaders, which handle cache coherency, and</li>
<li>followers, which serve most read requests</li>
</ul></li>
<li>Hadoop servers handle offline analysis and data mining</li>
<li>Multifeed servers assemble news feeds</li>
</ul>
<h3 id="data-collection">Data Collection</h3>
<p>The study considers two distinct data sources: - Fbflow is a
production monitoring system that samples packet headers from Facebook&#x2019;s
entire machine fleet. In production use, it aggregates statistics at a
per-minute granularity. - To collect high-fidelity data, they collect
traces by turning on port mirroring on the RSW and mirroring the full,
bi-directional traffic for a single server to collection server. Since
<code>tcpdump</code> is unable to handle more than approximately 1.5
Gbps of traffic, so in order to support line-rate traces, they employ a
custom kernel module that effectively pins all free RAM on the server
and uses it to buffer incoming packets.</p>
<p><em>Limitations:</em></p>
<ul>
<li>Using end hosts to capture and timestamp packets introduces
scheduler-based variations on timestamp accuracy.</li>
<li>Traffic can only be captured from a few hosts at a time without
risking drops in packet collection.</li>
<li>These constrains prevent evaluating effects like incast or
microbursts which lead to poor application performance.</li>
<li>Per-host packet dumps are anecdotal and ad hoc, relying on the
presence of an unused capture host on the same rack as the target.</li>
</ul>
<h3 id="provisioning">Provisioning</h3>
<p>The study quantifies the traffic intensity, locality and stability
across three different types of clusters: Hadoop, Frontend machines
serving Web requests and Cache.</p>
<h4 id="utilization">Utilization</h4>
<ul>
<li>The overall access link (i.e.&#xA0;links between hosts and their RSW)
utilization is quite low, with the average 1-minute link utilization
less than 1%.</li>
<li>Demand follows typical diurnal and day-of-the-week patterns,
although the magnitude of change is on the order of 2x.</li>
<li>Load varies considerably across clusters, with Hadoop clusters
having 5x average link utilization than Frontend clusters.</li>
<li>Utilization rises at higher levels of aggreagtion (between RSWs and
CSWs, and between CSWs and FCs).</li>
</ul>
<h4 id="locality-and-stability">Locality and stability</h4>
<ul>
<li>Hadoop traces show both rack- and cluster-level locality. This
pattern is consistent with previous observations in which traffic is
either rack-local or destined to one of roughly 1-10% of the hosts in
the cluster. From traffic matrix, they found Hadoop traffic is
cluster-local (more than 80%), as opposed to rack-local (14%).</li>
<li>Traffic patterns for other server classes are stable with lack of
rack-local traffic and large quantities of inter-datacenter traffic.
This contrasts previous observations.
<ul>
<li>Frontend cache followers primarily send traffic (response) to Web
servers, leading to high intra-cluster traffic (serving cache reads).
Load balancing spreads this traffic widely.</li>
<li>Cache leaders maintain coherency across clusters and the backing DB,
leading to intra- and inter-datacenter traffic.</li>
<li>Strong bipartite traffic pattern between Web servers and cache
followers in Webserver racks, within the cluster. This is because Web
servers talk primarily to cache servers and vice versa, and server of
different types are deployed in distinct racks leading to low intra-rack
traffic.</li>
</ul></li>
<li>Relative proportions of locality is stable, but the toal amount of
traffic may change due to diurnality</li>
</ul>
<p>The authors give a concrete argument for this locality. Loading the
Facebook news feed draws from a vast array of different objects in the
social graph: different people, relationships and events comprise a
large graph interconnected in a complicated fashion. This connectedness
means that the working set is unlikely to reduce even if users are
partitioned; the net result is a low cache hit rate within the rack,
leading to high intra-cluster traffic locality.</p>
<p><em>Implications for connection fabrics:</em></p>
<ul>
<li>The highly contrasting locality properties of the different clusters
imply a single homogeneous topology will either be over-provisioned in
some regions or congested in others. Non-uniform fabric technologies
that can deliver higher bandwidth to certain locations than others may
be useful.</li>
<li>Rapid reconfigurability may not be necessary because of the
stability of traffic patterns.</li>
<li>The lack of intra-rack locality suggest that RSWs that deliver less
than full non-blocking line-rate connectivity between all their ports
may be viable.</li>
</ul>
<h3 id="traffic-engineering">Traffic Engineering</h3>
<ul>
<li>Flow characteristics:
<ul>
<li>Most flows in Facebook&#x2019;s Hadoop cluster are short. The median flow
sends less than 1 KB and last less than a second.</li>
<li>Most flows are long-lived due to heavy use of connection pooling in
Facebook&#x2019;s internal services, and pooling is prevalent in cache
follower/leader nodes.</li>
<li>Most flows are active (actually transmit packets) only during
distinct millisecond-scale intervals with large intervening gaps. Thus,
flows are internally bursty.</li>
<li>Web server flows lie between cache flows and Hadoop flows.</li>
</ul></li>
<li>Facebook&#x2019;s load balancing is highly effective on timescales lasting
minutes to hours, leaving less room for traffic engineering. Hadoop (not
load balances) shows wide variation in flow rates and it is this
variability of traffic that existing traffic engineering schemes seek to
leverage.</li>
<li>Other flows (Web servers and cache) rates are stable, both over time
and by destination, due to combination of workload characteristics and
engineering effort. The offered load per second is roughly held constant
to a cache system - large increased in load would indicate presence of
relatively hot objects, which is actively monitored and mitigated.</li>
</ul>
<p><em>Heavy hitters:</em></p>
<p>Heavy hitters represent the minimum set of flows (or hosts, or racks
in the aggregated case) that is responsible for 50% of the observed
traffic volume (in bytes) over a fixed time period. Heavy hitters
signify an imbalance than can be acted upon. The study finds it
challenging to identify heavy hitters that persist with any frequently.
This situation results from a combination of effective load balancing
(little difference in size between a heavy hitter and the median flow)
and the relatively low long-term throughput of most flows (even heavy
flows can be quite bursty internally).</p>
<h3 id="switching">Switching</h3>
<ul>
<li>Hadoop traffic is bimodal: almost all packets are either MTU length
(1500 bytes) or TCP ACKs. Packets for other services have median size
less than 200 bytes.</li>
<li>While link utilization is low, the packet rate is high. A cache
server at 10% link utilization with a median packet size of 175 bytes
generates 85% of the packet rate of a fully utilized link sending
MTU-sized packets. Any per-packet operation may be stressed.</li>
<li>Hosts in Facebook&#x2019;s datacenter do not exhibit on/off arrival
pattern, even within Hadoop clusters. This maybe due to large number of
concurrent destinations.</li>
<li>The combination of a lack of on/off traffic, higher flow intensity,
and bursty individual flows increase buffer utilization and overruns.
Even though link utilization is on the order of 1% most of the time,
over 2/3 of the available shared buffer is utilized during each
10-microsecond interval. Careful buffer tuning is important moving
forward.</li>
<li>Web servers and cache hosts have 100s to 1000s of concurrent
connections, while Hadoop nodes have approximately 25 concurrent
connections on average.</li>
</ul>
<h2 id="strengths-weaknesses">Strengths &amp; Weaknesses</h2>
<h3 id="strengths">Strengths</h3>
<ul>
<li>This study presents network traffic characteristics in datacenter
for a hyperscalar internet service, along with some workload
information. This is invaluable for future work on designing network
fabrics to efficiently interconnect and manage the traffic within
datacenters. The study comments on the limited literature regarding
network traffic characteristics, and contrast the findings with previous
contributions on a number of points.</li>
<li>Such studies provide inspiration for future work in multiple ares of
datacenter fabrics, traffic engineering, SDN controllers, and
circuit/hybrid switching schemes. These studies are important to
understand the problems with the current standards, and think about ways
to improve on the existing strategies.</li>
<li>The study provides the state-of-the-art (at the time) connection
fabric used in data centers, and also tools for used for effective
networking monitoring and analysis at hyperscalar scales. At these
scales, conventional tools such as tcpdump cannot be used (mentioned
above).</li>
</ul>
<h3 id="weaknesses">Weaknesses</h3>
<ul>
<li>As the study mentions, some of the results and distribution graphs
are not representative of the actual traffic characteristics. This is a
challenging aspect of the study and we need to take their results in
faith. In some sections, the authors caution against examining the
specific distributions too carefully, as traces from other nodes or even
the same node at different times reveal different distributions.
Understandably, it is extremely hard to extract meaningful
generalization about network traffic behavior at such huge scales with
the current monitoring techniques.</li>
<li>The methodology imposes a few limitations on the scope of the study.
As mentioned before, using end hosts to capture and timestamp packets
introduces scheduler-based variations on timestamp accuracy. In
addition, one can only capture traffic from a few hosts at a time
without risking drops in packet collection. These constraints prevent
from evaluating effects like incasts or microbursts. Per-host packet
dumps are anecdotal and ad hoc, relying on the presence of an unused
capture host on the same rack as the target.</li>
</ul>
<h2 id="follow-on">Follow-On</h2>
<p>There are multiple immediate follow-on ideas from this work:</p>
<ul>
<li>Investigate the impact of using different communication fabrics with
variable degrees of oversubscription, and also using top of the rack
switches (RSWs) that deliver less than full non-blocking line-rate
connectivity to leverage the lack of rack-local traffic.</li>
<li>Hadoop shows wide variation in flow rates which existing network
traffic engineering schemes can leverage. Orchestra relies on temporal
and per-job variation to provide lower task completion times for
high-priority tasks. Hedera provides non-interfering route placement for
high bandwidth elephant flows that last for several seconds. One can
study the impact of using such mechanisms.</li>
<li>As utilization increases in the future, it might be through an
increase in the number of flows, in the size of packets, or both. Larger
packets with the same level of burstiness will use up more of the
buffer. A larger number of flows leads to a greater chance of multiple
flows sending bursts of packets simultaneously. Thus, we need to
investigate careful buffer tuning mechanisms.</li>
<li>Investigate new tools for effective networking monitoring and
analysis at hyperscalar scales.</li>
</ul>
<hr />
  </div>

  <script type="text/javascript" src="../js/mathjax_config.js"></script>
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-90310997-2', 'auto');
		ga('send', 'pageview');
	</script>
</body>
</html>
